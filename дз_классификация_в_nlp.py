# -*- coding: utf-8 -*-
"""КР_ДЗ_Классификация в NLP

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_5LuSa4izlCEJHIVKduS3a6qqC9tU9V0

Предварительно про PyTorch:
* [Про тензоры в pytorch](https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/tensor_tutorial.ipynb)
* [Про автоматическое дифференцирование и что такое .backwards()](https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/autograd_tutorial.ipynb)
* [Очень простая нейронка на pytorch](https://colab.research.google.com/drive/1RsZvw4KBGn5U5Aj5Ak7OG2pHx6z1OSlF)

#Загрузка данных
"""

!wget https://raw.githubusercontent.com/diptamath/covid_fake_news/main/data/Constraint_Train.csv

import pandas as pd

df = pd.read_csv('Constraint_Train.csv')

df.head()

"""# Методы sklearn

## модели из лекции
"""

from nltk.tokenize import word_tokenize
from tqdm import tqdm

import nltk
nltk.download('punkt')

sentences = [word_tokenize(text.lower()) for text in tqdm(df.tweet)] # токенизируем текст

"""### __model__ (Word2vec + LogisticRegression)"""

# Commented out IPython magic to ensure Python compatibility.
from gensim.models.word2vec import Word2Vec # векторизуем тексты с помощью словарных эмбендингов
# %time model_tweets = Word2Vec(sentences, workers=4, vector_size=300, min_count=3, window=5, epochs=15)

model_tweets.wv.most_similar('vaccine') # проверка на адекватность - смотрим близкие слова к слову "вакцина"

model_tweets.wv.most_similar('quarantine') # проверка на адекватность - смотрим близкие слова к слову "карантин"

model_tweets.init_sims() # нормируем вектора, чтобы они были в одинаковом пространстве

import numpy as np

def get_text_embedding(text):
    """создать эмбединг текста"""
    result = []
    for word in word_tokenize(text.lower()):
        if word in model_tweets.wv:
            result.append(model_tweets.wv[word])

    if len(result):
        result = np.sum(result, axis=0)
    else:
        result = np.zeros(300)
    return result

features = [get_text_embedding(text) for text in tqdm(df.tweet)] # вектора признаков

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(features, df.label, test_size=0.33, random_state=0)

model = LogisticRegression(random_state=0)
model.fit(X_train, y_train)

from sklearn.metrics import classification_report

predicted = model.predict(X_test)

print(classification_report(y_test, predicted, digits = 3))

"""### __model_1__ (CountVectorizer + LogisticRegression)"""

from sklearn.feature_extraction.text import CountVectorizer

vec = CountVectorizer()

bow = vec.fit_transform(df.tweet)

X1_train, X1_test, y1_train, y1_test = train_test_split(bow, df.label, test_size=0.33, random_state=0)
model_1 = LogisticRegression(random_state=0)
model_1.fit(X1_train, y1_train)

predicted_1 = model_1.predict(X1_test)
print(classification_report(y1_test, predicted_1))

"""## мои модели

### __model_1_clean__  без стоп-слов (CountVectorizer + LogisticRegression)
"""

import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
print(stopwords.words('english'))

from string import punctuation
punctuation

noise = stopwords.words('english') + list(punctuation)

vec_1 = CountVectorizer(stop_words=noise)
bow_1 = vec_1.fit_transform(df.tweet)
X2_train, X2_test, y2_train, y2_test = train_test_split(bow_1, df.label, test_size=0.33, random_state=0)
model_1_clean = LogisticRegression(random_state=0)
model_1_clean.fit(X2_train, y2_train)
predicted_2 = model_1_clean.predict(X2_test)
print(classification_report(y2_test, predicted_2))

"""### __ppl_clf_1__  (CountVectorizer + TfidfTransformer + TruncatedSVD + LogisticRegression)"""

df.info()

df.label.value_counts(dropna=False) # смотрим сбалансированность данных

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

le.fit(df.label)

le.transform( ['real', 'fake']) # пример расшифровки

y = pd.Series (le.transform(df.label))
y.head(3)

X=df.tweet

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)

print(df.shape[0])
print(X_train.shape)
print(X_test.shape)

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.feature_extraction import DictVectorizer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report, confusion_matrix

ppl_clf_1 = Pipeline([
    ('vect', CountVectorizer(stop_words=noise)),
    ('tfidf', TfidfTransformer()),
    ('tm', TruncatedSVD(n_components=100)),
    ('clf', LogisticRegression(random_state=0))])

ppl_clf_1.fit(X_train, y_train)

pred_ppl_clf_1 = ppl_clf_1.predict(X_test)
print(classification_report(pred_ppl_clf_1, y_test, target_names=['fake', 'real']))

ppl_clf_1.get_params().keys()

"""Далее подбираю оптимальные параметры, делаю это частями, иначе слишком долго отрабатывается код (более 2.5часов на GPU), колаб постоянно меня выкидывает"""

hyperparams_1 = {
    'vect__ngram_range': [(1, 1), (2, 2), (3, 3)],
    'vect__max_df': [0.85, 0.95],
    'vect__min_df': [1,2,3,4],
    'vect__stop_words': [None, noise],
    'vect__max_features': [None, 10, 100, 500, 1000, 5000, 10000],
    'vect__strip_accents': ['ascii', 'unicode', None],
}

grid1 = GridSearchCV(ppl_clf_1, hyperparams_1, cv=4, n_jobs=-1).fit(X_train, y_train)
print('Оптимальные параметры для grid1:')
print(grid1.best_score_, grid1.best_params_)

noise_cor = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't", '!', '"', '#', '$', '%', '&', "'", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\', ']', '^', '_', '`', '{', '|', '}', '~']

ppl_clf_1 = Pipeline([
    ('vect', CountVectorizer(max_df=0.95, max_features=None, min_df=3, ngram_range=(1, 1), stop_words=noise_cor, strip_accents='unicode')),
    ('tfidf', TfidfTransformer()),
    ('tm', TruncatedSVD(n_components=100)),
    ('clf', LogisticRegression(random_state=0))])

ppl_clf_1.fit(X_train, y_train)

pred_ppl_clf_1 = ppl_clf_1.predict(X_test)
print(classification_report(pred_ppl_clf_1, y_test, target_names=['fake', 'real']))

hyperparams_1 = {
    'tfidf__norm': ['l1', 'l2', None],
    'clf__class_weight':[None, 'balanced'],
    'clf__penalty':[None, 'l2', 'l1', 'elasticnet'],
    'clf__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],
}

grid1 = GridSearchCV(ppl_clf_1, hyperparams_1, cv=4, n_jobs=-1).fit(X_train, y_train)
print('Оптимальные параметры для grid1:')
print(grid1.best_score_, grid1.best_params_)

ppl_clf_1 = Pipeline([
    ('vect', CountVectorizer(max_df=0.95, max_features=None, min_df=3, ngram_range=(1, 1), stop_words=noise_cor, strip_accents='unicode')),
    ('tfidf', TfidfTransformer(norm=None)),
    ('tm', TruncatedSVD(n_components=100)),
    ('clf', LogisticRegression(class_weight='balanced', penalty='l2', solver='lbfgs', random_state=0))])

ppl_clf_1.fit(X_train, y_train)

pred_ppl_clf_1 = ppl_clf_1.predict(X_test)
print(classification_report(pred_ppl_clf_1, y_test, target_names=['fake', 'real'], digits = 3))

"""### RandomizedSearchCV"""

from sklearn.model_selection import RandomizedSearchCV

from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import MultinomialNB

from tqdm import tqdm

import warnings
warnings.filterwarnings('ignore')

from sklearn.neighbors import KNeighborsClassifier

models=[
      {'name':'Lr',"model": LogisticRegression(random_state=0),
       'params':
              {'C':np.linspace(0, 10, 5),
                'penalty':['l1', 'l2', 'elasticnet', 'none'],
                'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']
              }
      },
      {'name':'SVC',"model": SVC(random_state=0),
       'params':
              {'C': np.linspace(0, 10, 5),
               'kernel':['linear', 'poly', 'rbf', 'sigmoid'],
               'degree':[1,2,3,5]
               }
       },
      {'name':'RF',"model": RandomForestClassifier(random_state=0),
       'params':
              {'n_estimators':[10,25,50,100,150,200],
               'criterion':['gini', 'entropy'],
               'max_depth':[3,5,7,9,11],
               'min_samples_leaf':[1,2,3,5]
               }
       },
      {'name':'KNN',"model": KNeighborsClassifier(),
       'params':
              {'n_neighbors':list(range(1,30)),
               'weights': ['uniform', 'distance'],
               'p':[1,2,3],
               'metric':['euclidean', 'minkowski']
               }
       },
      {'name':'DT',"model": DecisionTreeClassifier(random_state=0),
       'params':
              {'criterion':['gini', 'entropy'],
               'max_depth':[3,5,7,9,11],
               'min_samples_split':[2,3,4,5,7,9],
               'min_samples_leaf':[1,2,3,5]
               }
       }
]

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# 
# res = []
# for v in tqdm(models):
#     res.append((v['name'], RandomizedSearchCV(v['model'], v['params'],cv=4, random_state=0).fit(X2_train, y2_train)))

for r in res:
    print(r[0], r[1].best_score_, r[1].best_params_)

"""#### model_1_clean_1 (CountVectorizer + SVC)"""

model_1_clean_1 = SVC(kernel='rbf', degree=5, C=2.5, random_state=0)
model_1_clean_1.fit(X2_train, y2_train)
predicted_3 = model_1_clean_1.predict(X2_test)
print(classification_report(y2_test, predicted_3))

"""## сводная таблица по моделям"""

report = classification_report(y_test, predicted, output_dict=True)
df_r = pd.DataFrame(report)
test=pd.Series(round(df_r[2:3]['macro avg'],3), name='model (Word2vec + LogisticRegression)')
test=pd.DataFrame(test)

report_1 = classification_report(y1_test, predicted_1, output_dict=True)
df_r_1 = pd.DataFrame(report_1)
test_1=pd.Series(round(df_r_1[2:3]['macro avg'],3), name='model_1 (CountVectorizer + LogisticRegression)')
test_1=pd.DataFrame(test_1)

report_2 = classification_report(y2_test, predicted_2, output_dict=True)
df_r_2 = pd.DataFrame(report_2)
test_2=pd.Series(round(df_r_2[2:3]['macro avg'],3), name='model_1_clean без стоп-слов (CountVectorizer + LogisticRegression)')
test_2=pd.DataFrame(test_2)

report_3 = classification_report(y_test, pred_ppl_clf_1, output_dict=True)
df_r_3 = pd.DataFrame(report_3)
test_3=pd.Series(round(df_r_3[2:3]['macro avg'],3), name='ppl_clf_1 (CountVectorizer + TfidfTransformer + TruncatedSVD + LogisticRegression)')
test_3=pd.DataFrame(test_3)

report_4 = classification_report(y2_test, predicted_3, output_dict=True, digits = 3)
df_r_4 = pd.DataFrame(report_4)
test_4=pd.Series(round(df_r_4[2:3]['macro avg'],3), name='model_1_clean_1 (CountVectorizer + SVC)')
test_4=pd.DataFrame(test_4)

test_concat = pd.concat([test, test_1, test_2,test_3, test_4], axis=1)

test_concat.style.format('{:.3f}').highlight_max(color = 'lightgreen', axis = 1)

"""# Методы на PyTorch

## модель из лекции (PyTorch + LSTM)
"""

labels = (df.label == 'real').astype(int).to_list() # переводим метки в числа

labels[:5]

"""Нужно заранее задать размер для макксимальной длины предложений."""

from nltk.tokenize import word_tokenize
from tqdm import tqdm

import nltk
nltk.download('punkt')

import numpy as np

token_lists = [word_tokenize(text.lower()) for text in df.tweet] # токенизируем тексты
max_len = len(max(token_lists, key=len))

max_len

"""Это слишком много. Но какая длина обычно?"""

from collections import Counter
fd = Counter([len(tokens) for tokens in token_lists]) # смотрим какие длины чаще встречаются

fd.most_common(10)

"""Зададим максимум 200.

Возьмём те же w2v эмбеддинги.
"""

sentences = [word_tokenize(text.lower()) for text in tqdm(df.tweet)]

# Commented out IPython magic to ensure Python compatibility.
from gensim.models.word2vec import Word2Vec
# %time model_tweets = Word2Vec(sentences, workers=4, vector_size=300, min_count=3, window=5, epochs=15)

def get_word_embedding(tokens, max_len):
    result = []
    for i in range(max_len):
        if i < len(tokens):
            word = tokens[i]
            if word in model_tweets.wv:
                result.append(model_tweets.wv[word])
            else:
                result.append(np.zeros(300))
        else:
            result.append(np.zeros(300))
    return result

features = [get_word_embedding(text, 200) for text in tqdm(token_lists)] # список векторов каждого слова в тексте

features[:2]

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.33)

import torch
import torch.nn as nn
import torch.optim as optim

len(features[0][0])

len(X_train)

len(X_train[0])

len(X_train[0][0])

in_data = torch.tensor(X_train).float()
targets = torch.tensor(y_train).float()

in_data.shape

# Сети с долговременной кратковременной памятью (LSTM) представляют собой особый вид RNN, которые способны изучать долгосрочные зависимости.
class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()
        self.lstm = nn.LSTM(300, 100) # на вход - вектор размерностью 300, скрытый слой задали 100
        self.out = nn.Linear(100, 1)

    def forward(self, x):
        embeddings, (shortterm, longterm) = self.lstm(x.transpose(0, 1)) # longterm - линейный слой долгосрочной памяти
        prediction = torch.sigmoid(self.out(longterm))
        return prediction


net = Net()
print(net)

optimizer = optim.SGD(net.parameters(), lr=0.01)
criterion = nn.BCELoss()

def train_one_epoch(in_data, targets, batch_size=16):
    for i in tqdm(range(0, in_data.shape[0], batch_size)):
        batch_x = in_data[i:i + batch_size]
        batch_y = targets[i:i + batch_size]
        optimizer.zero_grad()
        output = net(batch_x)
        loss = criterion(output.reshape(-1), batch_y)
        loss.backward()
        optimizer.step()
    print(loss)

train_one_epoch(in_data, targets)

"""Что получилось?"""

in_data_test = torch.tensor(X_test).float()
targets_test = torch.tensor(y_test).float()

with torch.no_grad():
    output = net(in_data_test).reshape(-1)

result = (output > 0.5) == targets_test

result.sum().item() / len(result)

"""Но такую модель надо учить дольше(

## оптимизация модели из лекции

### увеличиваем количество эпох до 10
"""

for i in range(10):
  train_one_epoch(in_data, targets)

in_data_test = torch.tensor(X_test).float()
targets_test = torch.tensor(y_test).float()

with torch.no_grad():
    output = net(in_data_test).reshape(-1)

result = (output > 0.5) == targets_test

result.sum().item() / len(result)

"""### заменой оптимизатора на Adam

Замена оптимизатора SGD на оптимизатор __Rectified Adam PyTorch__.

Исправленный оптимизатор Адама Pytorch — это альтернатива оптимизатору Адама, который пытается решить проблему плохой сходимости Адама.
Он также используется для исправления изменений скорости адаптивного обучения.
"""

optimizer_RAdam = optim.RAdam(net.parameters(),lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0)

criterion = nn.BCELoss()

def train_one_epoch_RAdam(in_data, targets, batch_size=16):
    for i in tqdm(range(0, in_data.shape[0], batch_size)):
        batch_x = in_data[i:i + batch_size]
        batch_y = targets[i:i + batch_size]
        optimizer_RAdam.zero_grad()
        output = net(batch_x)
        loss = criterion(output.reshape(-1), batch_y)
        loss.backward()
        optimizer_RAdam.step()
    print(loss)

for i in range(10):
  train_one_epoch_RAdam(in_data, targets)

in_data_test = torch.tensor(X_test).float()
targets_test = torch.tensor(y_test).float()

with torch.no_grad():
    output = net(in_data_test).reshape(-1)

result = (output > 0.5) == targets_test

result.sum().item() / len(result)

"""### добавление скрытых слоёв"""

# Добавим еще два скрытых слоя
class Net_1(nn.Module):

    def __init__(self):
        super(Net_1, self).__init__()
        self.lstm = nn.LSTM(300, 100)
        self.Linear = nn.Linear(100, 100)
        self.Linear = nn.Linear(100, 100)
        self.out = nn.Linear(100, 1)

    def forward(self, x):
        embeddings, (shortterm, longterm) = self.lstm(x.transpose(0, 1))
        prediction = torch.sigmoid(self.out(longterm))
        return prediction


net_1 = Net_1()
print(net_1)

optimizer_RAdam_1 = optim.RAdam(net_1.parameters(),lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0)

criterion = nn.BCELoss()

def train_one_epoch_RAdam_1(in_data, targets, batch_size=16):
    for i in tqdm(range(0, in_data.shape[0], batch_size)):
        batch_x = in_data[i:i + batch_size]
        batch_y = targets[i:i + batch_size]
        optimizer_RAdam_1.zero_grad()
        output = net_1(batch_x)
        loss = criterion(output.reshape(-1), batch_y)
        loss.backward()
        optimizer_RAdam_1.step()
    print(loss)

#for i in range(10): # включился лимит на ускоритель, поэтому делаю 1 эпоху, десять всё равно мало, для видимого результата надо добавлять больше 10 эпох.
train_one_epoch_RAdam_1(in_data, targets)

in_data_test = torch.tensor(X_test).float()
targets_test = torch.tensor(y_test).float()

with torch.no_grad():
    output = net_1(in_data_test).reshape(-1)

result = (output > 0.5) == targets_test

result.sum().item() / len(result)

"""### добавление функции активации скрытого слоя"""

# Добавим функцию активации скрытого слоя ReLU
class Net_1(nn.Module):

    def __init__(self):
        super(Net_1, self).__init__()
        self.lstm = nn.LSTM(300, 100)
        self.Linear = nn.Linear(100, 100)
        self.ReLU=nn.ReLU()
        self.Linear = nn.Linear(100, 100)
        self.ReLU=nn.ReLU()
        self.out = nn.Linear(100, 1)

    def forward(self, x):
        embeddings, (shortterm, longterm) = self.lstm(x.transpose(0, 1))
        prediction = torch.sigmoid(self.out(longterm))
        return prediction


net_1 = Net_1()
print(net_1)

optimizer_RAdam_1 = optim.RAdam(net_1.parameters(),lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0)

criterion = nn.BCELoss()

def train_one_epoch_RAdam_1(in_data, targets, batch_size=16):
    for i in tqdm(range(0, in_data.shape[0], batch_size)):
        batch_x = in_data[i:i + batch_size]
        batch_y = targets[i:i + batch_size]
        optimizer_RAdam_1.zero_grad()
        output = net_1(batch_x)
        loss = criterion(output.reshape(-1), batch_y)
        loss.backward()
        optimizer_RAdam_1.step()
    print(loss)

for i in range(10):
  train_one_epoch_RAdam_1(in_data, targets)

in_data_test = torch.tensor(X_test).float()
targets_test = torch.tensor(y_test).float()

with torch.no_grad():
    output = net_1(in_data_test).reshape(-1)

result = (output > 0.5) == targets_test

result.sum().item() / len(result)

"""### + 10 эпох"""

for i in range(10):
  train_one_epoch_RAdam_1(in_data, targets)

with torch.no_grad():
    output = net_1(in_data_test).reshape(-1)

result = (output > 0.5) == targets_test

result.sum().item() / len(result)