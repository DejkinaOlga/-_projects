# -*- coding: utf-8 -*-
"""КР_ДЗ_Введение в сверточные НС

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14ohfkHGitTwcg5wnT9b85kynJkoFWDKR

# Домашнее задание. Свёрточные сети

Здесь вам предстоит построить и обучить свою первую свёрточную сеть для классификации изображений на данных CIFAR10.
"""

import tensorflow as tf

from tqdm import tqdm_notebook

"""## Данные

CIFAR10
* 60000 RGB изображений размером 32x32x3
* 10 классов: самолёты, собаки, рыбы и т.п.

<img src="https://www.samyzaf.com/ML/cifar10/cifar1.jpg" style="width:60%">

Загрузите данные, разделите их на обучающую и тестовую выборки. Размер тестовой выборки должен быть $10^4$.
"""

import numpy as np
from keras.datasets import cifar10
from sklearn.model_selection import train_test_split
(X_train, y_train), (X_test, y_test) = cifar10.load_data()
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=10**4, random_state=42)

class_names = np.array(['airplane','automobile ','bird ','cat ','deer ','dog ','frog ','horse ','ship ','truck'])

print (X_train.shape,y_train.shape)

"""Прежде чем приступать к основной работе, стоит убедиться что загруженно именно то, что требовалось:"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

plt.figure(figsize=[12,10])
for i in range(12):
    plt.subplot(3, 4, i + 1)
    plt.xlabel(class_names[y_train[i, 0]])
    plt.imshow(X_train[i])

"""## Подготовка данных

Сейчас каждый пиксель изображения закодирован тройкой чисел (RGB) __от 0 до 255__. Однако лучше себя показывает подход, где значения входов нейросети распределены недалеко от 0.

Давайте приведём все данные в диапазон __`[0, 1]`__ — просто разделим на соответствующий коэффициент:
"""

X_train = X_train / 255
X_val = X_val / 255
X_test = X_test / 255

"""Исполните код ниже для проверки, что все выполнено корректно."""

assert np.shape(X_train) == (40000, 32, 32, 3), "data shape should not change"
assert 0.9 <= max(map(np.max, (X_train, X_val, X_test))) <= 1.05
assert 0.0 <= min(map(np.min, (X_train, X_val, X_test))) <= 0.1
assert len(np.unique(X_test / 255.)) > 10, "make sure you casted data to float type"

"""## Архитектура сети

Для начала реализуйте простую нейросеть:
1. принимает на вход картинки размера 32 x 32 x 3;
2. вытягивает их в вектор (`keras.layers.Flatten`);
3. пропускает через 1 или 2 полносвязных слоя;
4. выходной слой отдает вероятности принадлежности к каждому из 10 классов.

Создайте полносвязную сеть:
"""

import keras
from keras import layers as L
from keras import backend as K
from keras.layers import Dense
from keras.layers import Flatten
from keras.models import Sequential

model = Sequential([
    L.Flatten(input_shape=(32, 32, 3)), # преобразует формат изображения в 1d-массив из 32*32*3 = 3072 пикселей.
    L.Dense(64, activation="relu"), # первый слой Dense содержит 64 узла (или нейронов)
    L.Dense(128, activation="relu"), # второй слой Dense содержит 128 узла (или нейронов)
    L.Dense(10, activation="sigmoid") # слой с 10 узлами, возвращает массив из десяти вероятностных оценок, сумма которых равна 1.
])
model.summary()

dummy_pred = model.predict(X_train[:20])
assert dummy_pred.shape == (20, 10)
assert np.allclose(dummy_pred.sum(-1), dummy_pred.sum(1))
print("Успех!")

keras.utils.plot_model(model, "my_first_model.png", show_shapes=True)

"""## Обучение сети

**Задание 1.1 (обязательно)** Будем минимизировать многоклассовую кроссэкнропию с помощью __sgd__. Вам нужно получить сеть, которая достигнет __не менее 45%__ __accuracy__ на тестовых данных.

__Важно:__ поскольку в y_train лежат номера классов, Керасу нужно либо указать sparse функции потерь и метрики оценки качества классификации (`sparse_categorical_crossentropy` и `sparse_categorical_accuracy`), либо конвертировать метки в one-hot формат.

### Полезные советы
* `model.compile` позволяет указать, какие метрики вы хотите вычислять.
* В `model.fit` можно передать валидационную выборку (`validation_data=[X_val, y_val]`), для отслеживания прогресса на ней. Также рекомендуем сохранять результаты в [tensorboard](https://keras.io/callbacks/#tensorboard) или [wandb](https://docs.wandb.ai/integrations/jupyter). **Важно: логи tensorboard не получится без боли посмотреть через colab.** Workaround: скачать логи и запустить tensorboard локально или помучаться [с этим](https://stackoverflow.com/questions/47818822/can-i-use-tensorboard-with-google-colab).
* По умолчанию сеть учится 1 эпоху. Совсем не факт, что вам этого хватит. Число эпох можно настроить в методе `fit` (`epochs`).
* Ещё у Кераса есть много [полезных callback-ов](https://keras.io/callbacks/), которые можно попробовать. Например, автоматическая остановка или подбор скорости обучения.
"""

y_train, y_val = (keras.utils.to_categorical(y) for y in (y_train, y_val))

model.compile(optimizer='SGD',
              loss=keras.losses.categorical_crossentropy,
              metrics=['accuracy'])

# model_checkpoint = keras.callbacks.ModelCheckpoint(filepath='model_best_{epoch}.h5',
                                             #monitor='val_accuracy',
                                             #verbose=1,
                                             #save_best_only=True,
                                             #save_weights_only=False,
                                             #mode='auto',
                                             #save_freq='epoch'
                                             #)

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

import datetime

log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

model.fit(X_train, y_train,
          batch_size=64,
          epochs=15,
          validation_data=[X_val, y_val],
          callbacks=[tensorboard_callback])

y_pred = model.predict(X_test)
y_pred_labels = np.argmax(y_pred, axis=1)

"""А теперь можно проверить качество вашей сети, выполнив код ниже:"""

from sklearn.metrics import accuracy_score
test_acc = accuracy_score(y_test, np.argmax(model.predict(X_test), axis=1))
print("\n Test_acc =", test_acc),
assert test_acc > 0.45, "Not good enough. Back to the drawing board :)"
print(" Not bad!")

"""## Карманная сверточная сеть

**Задание 1.2 (обязательно)** Реализуйте небольшую свёрточную сеть. Совсем небольшую:
1. Входной слой
2. Свёртка 3x3 с 10 фильтрами
3. Нелинейность на ваш вкус
4. Max-pooling 2x2
5. Вытягиваем оставшееся в вектор (Flatten)
6. Полносвязный слой на 100 нейронов
7. Нелинейность на ваш вкус
8. Выходной полносвязный слой с softmax

Обучите её так же, как и предыдущую сеть. Если всё хорошо, у вас получится accuracy не меньше __50%__.
"""

from keras.layers import Conv2D, MaxPooling2D

model = Sequential([
    Conv2D(10, (3,3), padding='same', activation='relu', input_shape=(32, 32, 3)), #10 фильтров с ядрами 3х3 пиксела каждый; выходная карта признаков на каждом канале 32*32
    MaxPooling2D((2, 2), strides=2),
    Conv2D(32, (3,3), padding='same', activation='relu'),
    MaxPooling2D((2, 2), strides=2),
    Conv2D(64, (3,3), padding='same', activation='relu'),
    MaxPooling2D((2, 2), strides=2),
    Flatten(),
    Dense(100, activation='relu'),
    Dense(10,  activation='softmax')
])

model.compile(optimizer='SGD',
              loss=keras.losses.categorical_crossentropy,
              metrics=['accuracy'])

model.fit(X_train, y_train,
          batch_size=64,
          epochs=15,
          validation_data=[X_val, y_val],
          callbacks=[tensorboard_callback])

y_pred = model.predict(X_test)
y_pred_labels = np.argmax(y_pred, axis=1)

"""Давайте посмотрим, смогла ли карманная сверточная сеть побить заданный порог по качеству:"""

from sklearn.metrics import accuracy_score
test_acc = accuracy_score(y_test, np.argmax(model.predict(X_test), axis=1))
print("\n Test_acc =", test_acc)
assert test_acc > 0.50, "Not good enough. Back to the drawing board :)"
print(" Not bad!")

"""## Учимся учить

А теперь научимся сравнивать кривые обучения моделей — зависимости значения accuracy от количества итераций.

Вам потребуется реализовать _экспериментальный стенд_ — вспомогательный код, в который вы сможете подать несколько архитектур и методов обучения, чтобы он их обучил и вывел графики кривых обучения. Это можно сделать с помощью `keras.callbacks` — `TensorBoard` или `History`.

Будьте морально готовы, что на обучение уйдёт _много времени_. Даже если вы ограничитесь 10 эпохами. Пока идёт обучение, вы можете переключиться на другие задания или заняться чем-нибудь приятным: поспать, например.

**Задание 1.3 (опционально)** Попробуйте использовать различные методы оптимизации (sgd, momentum, adam) с параметрами по умолчанию. Какой из методов работает лучше?

Для удобства напишем класс Evaluator, который принимает в себя дикты виды {имя_оптимайзера: инстанс}, {имя модели: инстанс} и обучает всевозможные комбинации моделей с оптимайзерами при помощи метода fit (попутно записывая логи отдельно для каждой модели). Также пригодится метод evaluate для отображения итоговых скоров.

Пользоваться классом не обязательно. По умолчанию класс использует tensorboard. Если вы выше использовали wandb -- советуем дописать callback.
"""

class Evaluator(list):
    def __init__(self, models, optimizers='adam', loss=keras.losses.categorical_crossentropy,
                 metrics=[keras.metrics.categorical_accuracy]):
        '''
            models: dict {name: model}
            optimizers: list of optimizers or just one optimizer
        '''
        if not isinstance(models, dict):
            models = {'single_model': models}
        if not isinstance(optimizers, dict):
            optimizers = {str(optimizers.__class__): optimizers}
        super().__init__([(model_name, keras.models.clone_model(model), optimizer_name, optimizer)
                          for model_name, model in models.items()
                          for optimizer_name, optimizer in optimizers.items()])
        for _, model, _, optimizer in self:
            model.compile(optimizer=optimizer, loss=loss, metrics=metrics)

    def fit(self, X, y, validation_data=(), max_epochs=55, verbose=0, callbacks=[], batch_size=32):
        if not isinstance(callbacks, list):
            callbacks = [callbacks]
        for model_name, model, optimizer_name, optimizer in tqdm_notebook(self):
            model.fit(X, y, validation_data=validation_data or None, epochs=max_epochs, verbose=verbose,
                      batch_size=batch_size, callbacks=callbacks + [keras.callbacks.TensorBoard(
                          log_dir='./logs/{}_{}'.format(model_name, optimizer_name))])

    def fit_generator(self, X, y, validation_data=(), max_epochs=55, verbose=1, callbacks=[], batch_size=32):
        datagen = keras.preprocessing.image.ImageDataGenerator(
            rotation_range=20,
            width_shift_range=0.2,
            height_shift_range=0.2,
            horizontal_flip=True
        )
        if not isinstance(callbacks, list):
            callbacks = [callbacks]
        for model_name, model, optimizer_name, optimizer in tqdm_notebook(self):
            model.fit_generator(datagen.flow(X, y, batch_size=batch_size), epochs=max_epochs,
                validation_data=validation_data or None, verbose=verbose,
                callbacks=callbacks + [keras.callbacks.TensorBoard(
                    log_dir='./logs/{}_{}'.format(model_name, optimizer_name))])

    def evaluate(self, X, y, metric):
        for model_name, model, optimizer_name, _ in self:
            print('Final score of {}_{} is {}'.format(model_name, optimizer_name,
                  metric(y_test, np.argmax(model.predict(X_test), axis=1))))

!rm -rf ./logs

optimizers = {'optimizer_adam': 'adam','optimizer_sgd': 'sgd'
}

evaluator = Evaluator(model, optimizers=optimizers)
evaluator.fit(X_train, y_train, validation_data=(X_val, y_val))
evaluator.evaluate(X_test, y_test, accuracy_score)

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs/

"""Оба метода показывают переобучение, у адама оно намного сильнее. После 16-й эпохи точность адома на тесте ухудшается по сравнению с sgd

**Задание 1.4 (опционально)** Добавьте нормализацию по батчу (`BatchNormalization`) между свёрткой и активацией. Попробуйте использовать несколько нормализаций — в свёрточных и полносвязных слоях.

Для удобства реализуем класс Models, который по сути будет являться списком моделей с двумя методами: add (добавить слой ко всем моделям) и add_create (создать новую модель на основе базовой с дополнительным слоем). Пользоваться им необязательно, но вдруг :)
"""

from keras.layers import BatchNormalization

from collections import OrderedDict

# class Models(OrderedDict):
    #def __init__(self, models):
        #if not isinstance(models, dict):
            #models = OrderedDict({'base_model': models})
        #super().__init__(models)

    #def add(self, layer):
        #for name, model in self.items():
            #model.add(layer)

    #def add_create(self, name, layer):
        #base_model = next(iter(self.items()))[1]
        #new_model = keras.models.clone_model(base_model)
        #new_model.add(layer)
        #self.update({name: new_model})

    #def add_update(self, name, layer):
        #base_model = self[next(reversed(self))]
        #new_model = keras.models.clone_model(base_model)
        #new_model.add(layer)
        #self.update({name: new_model})

# Example of usage
# models = Models(keras.Sequential())
# models.add(L.InputLayer(input_shape=(32, 32, 3)))
# models.add(L.Convolution2D(filters=10, kernel_size=(3, 3)))
# models.add(L.MaxPooling2D())
# models.add_create('conv_batchnorm', L.BatchNormalization())
# models.add(L.Activation('relu'))
# ...

model_norm = Sequential([
    Conv2D(10, (3,3), padding='same', activation='relu', input_shape=(32, 32, 3)), #10 фильтров с ядрами 3х3 пиксела каждый; выходная карта признаков на каждом канале 32*32
    BatchNormalization(),
    MaxPooling2D((2, 2), strides=2),
    BatchNormalization(),
    Conv2D(32, (3,3), padding='same', activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2), strides=2),
    BatchNormalization(),
    Conv2D(64, (3,3), padding='same', activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2), strides=2),
    BatchNormalization(),
    Flatten(),
    BatchNormalization(),
    Dense(100, activation='relu'),
    BatchNormalization(),
    Dense(10,  activation='softmax')
])

model_norm.summary()

model_norm.compile(optimizer='SGD',
              loss=keras.losses.categorical_crossentropy,
              metrics=['accuracy'])

model_norm.fit(X_train, y_train,
          batch_size=64,
          epochs=15,
          validation_data=[X_val, y_val],
          callbacks=[tensorboard_callback])

from sklearn.metrics import accuracy_score
test_acc = accuracy_score(y_test, np.argmax(model_norm.predict(X_test), axis=1))
print("\n Test_acc =", test_acc)
assert test_acc > 0.50, "Not good enough. Back to the drawing board :)"
print(" Not bad!")

"""В результате добавления нормализации по батчу метрика увеличилась с 0.5731 до 0.6693

**Задание 1.5 (опционально)** Посмотрите на batch_size (параметр model.fit) - при большем батче модель будет быстрее проходить эпохи, но с совсем огромным батчом вам потребуется больше эпох для сходимости (т.к. сеть делает меньше шагов за одну эпоху).
Найдите такое значение, при котором модель быстрее достигает точности 55%. **Hint**: используйте early stopping callback.
"""

from sklearn.metrics import accuracy_score

batch_sizes_list = [4, 8, 16, 32, 64, 128]
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.01, verbose=1, mode='max', baseline=0.55,restore_best_weights=True)
for batch_size in batch_sizes_list:
    model.fit(X_train, y_train,
          batch_size=batch_size,
          epochs=5,
          validation_data=[X_val, y_val],
          callbacks=[early_stopping])
    test_acc = accuracy_score(y_test, np.argmax(model.predict(X_test), axis=1))
    print('Batch size: ' + str(batch_size))
    print('Accuracy: ' + str(test_acc))

"""**Задание 1.6 (опционально)** Попробуйте найти такую комбинацию метода обучения и нормализации, при которой сеть имеет наилучшую кривую обучения. Поясните, что вы понимаете под "наилучшей" кривой обучения."""

evaluator = Evaluator(model_norm, optimizers=optimizers)
evaluator.fit(X_train, y_train, validation_data=(X_val, y_val))
evaluator.evaluate(X_test, y_test, accuracy_score)

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs/

"""Адам работает лучше чем SGD, но есть сильное переобучение.Батч-нормализация даёт значительное улучшение. Лучший результат у батч-норма на сверточных и полносвязных слоях.

## Свёрточная нейросеть здорового человека

**Задание 1.7 (обязательно попытаться)** Наигравшись выше, обучим большую свёрточную сеть, которая даст на тестовой выборке __accuracy больше 80%__. В этом задании вам потребуется провести эксперименты, сравнив их между собой в конце. Возможно, может быть несколько проще, если писать выводы во время или сразу после каждого эксперимента, после чего сделать общие выводы.

Рекомендуем начать с лучшей модели предыдущего задания и постепенно её улучшать. Вы можете использовать всё, что угодно: любые активации, сколь угодно большие свёрточные слои и глубокие сети. Единственное ограничение: __нельзя использовать предобученные сети и дополнительные данные__.

### Полезные советы
* Для начала, неплохо бы научить что-нибудь побольше, чем 10 фильтров 3x3.
* __Главное правило: одно изменение на эксперимент__. Если у вас есть 2 идеи по улучшению сети, сначала попробуйте их независимо. Может оказаться, что одно из них дало __+10%__ точности а другое __-7%__. А вы так и будете думать, что сделали 2 полезных изменения которые в сумме дают __+3%__. Если какая-то идея не работает — даже если она вам нравится - опишите ее и выкидывайте из дальнейших экспериментов.
* __Be careful or you will dropout__. Дропаут (`L.Dropout`) может позволить вам обучить в несколько раз бОльшую сеть без переобучения, выжав несколько процентов качества. Это круто, но не стоит сразу ставить dropout 50%. Во-первых, слишком сильный дропаут только ухудшит сеть (underfitting). Во-вторых, даже если дропаут улучшает качество, он замедляет обучение. Рекомендуем начинать с небольшого дропаута, быстро провести основные эксперименты, а потом жахнуть в 2 раза больше нейронов и дропаута ~~на ночь~~.
* __Аугментация данных__. Если котика слегка повернуть и подрезать (простите), он всё равно останется котиком. А в керасе есть [удобный класс](https://keras.io/preprocessing/image/), который поставит подрезание котиков на поток. Ещё можно сделать этот трюк в тесте: вертим картинку 10 раз, предсказываем вероятности и усредняем. Только один совет: прежде, чем учить, посмотрите глазами на аугментированные картинки. Если вы сами не можете их различить, то и сеть не сможет.
* __Don't just stack more layers__. Есть более эффективные способы организовать слои, чем простой Sequential. Вот пара идей: [Inception family](https://hacktilldawn.com/2016/09/25/inception-modules-explained-and-implemented/), [ResNet family](https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035?gi=9018057983ca), [Densely-connected convolutions](https://arxiv.org/abs/1608.06993). Только не копируйте архитектуру подчистую — вам скорее всего хватит меньшего размера.
* __Долго != плохо__. Более глубокие архитектуры обычно требуют бОльше эпох до сходимости. Это значит, что в первые несколько эпох они могут быть хуже менее глубоких аналогов. Дайте им время, запаситесь чаем и обмажьтесь batch-norm-ом.

### увеличение начального количества фильтров до 20
"""

model_res = Sequential([
    Conv2D(20, (3,3), padding='same', activation='relu', input_shape=(32, 32, 3)), #20 фильтров с ядрами 3х3 пиксела каждый; выходная карта признаков на каждом канале 32*32
    BatchNormalization(),
    MaxPooling2D((2, 2), strides=2),
    BatchNormalization(),
    Conv2D(32, (3,3), padding='same', activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2), strides=2),
    BatchNormalization(),
    Conv2D(64, (3,3), padding='same', activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2), strides=2),
    BatchNormalization(),
    Flatten(),
    BatchNormalization(),
    Dense(100, activation='relu'),
    BatchNormalization(),
    Dense(10,  activation='softmax')
])

model_res.compile(optimizer='SGD',
              loss=keras.losses.categorical_crossentropy,
              metrics=['accuracy'])

model_res.fit(X_train, y_train,
          batch_size=64,
          epochs=15,
          validation_data=[X_val, y_val],
          callbacks=[tensorboard_callback])

from sklearn.metrics import accuracy_score
test_acc = accuracy_score(y_test, np.argmax(model_res.predict(X_test), axis=1))
print("\n Test_acc =", test_acc)
assert test_acc > 0.6693, "Not good enough. Back to the drawing board :)"
print(" Not bad!")
print("\n Изменение модели дало +", round(((test_acc-0.6693)/0.6693)*100),"% точности")

"""### добавление слоя Dropout"""

model_res = Sequential([
    Conv2D(20, (3,3), padding='same', activation='relu', input_shape=(32, 32, 3)), #20 фильтров с ядрами 3х3 пиксела каждый; выходная карта признаков на каждом канале 32*32
    BatchNormalization(),
    MaxPooling2D((2, 2), strides=2),
    BatchNormalization(),
    Conv2D(32, (3,3), padding='same', activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2), strides=2),
    BatchNormalization(),
    Conv2D(64, (3,3), padding='same', activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2), strides=2),
    BatchNormalization(),
    Flatten(),
    BatchNormalization(),
    Dense(100, activation='relu'),
    BatchNormalization(),
    L.Dropout(0.3), # исключающий слой для предотвращения переобучения, случайным образом отбрасывает 30% существующих соединений.
    Dense(10,  activation='softmax')
])

model_res.compile(optimizer='SGD',
              loss=keras.losses.categorical_crossentropy,
              metrics=['accuracy'])

model_res.fit(X_train, y_train,
          batch_size=64,
          epochs=15,
          validation_data=[X_val, y_val],
          callbacks=[tensorboard_callback])

from sklearn.metrics import accuracy_score
test_acc = accuracy_score(y_test, np.argmax(model_res.predict(X_test), axis=1))
print("\n Test_acc =", test_acc)
assert test_acc > 0.6939, "Not good enough. Back to the drawing board :)"
print(" Not bad!")
print("\n Изменение модели дало +", round(((test_acc-0.6939)/0.6939)*100),"% точности")

model_res = Sequential([
    Conv2D(20, (3,3), padding='same', activation='relu', input_shape=(32, 32, 3)), #20 фильтров с ядрами 3х3 пиксела каждый; выходная карта признаков на каждом канале 32*32
    BatchNormalization(),
    MaxPooling2D((2, 2), strides=2),
    BatchNormalization(),
    Conv2D(32, (3,3), padding='same', activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2), strides=2),
    BatchNormalization(),
    Conv2D(64, (3,3), padding='same', activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2), strides=2),
    BatchNormalization(),
    Flatten(),
    BatchNormalization(),
    Dense(100, activation='relu'),
    BatchNormalization(),
    L.Dropout(0.1), # исключающий слой для предотвращения переобучения, случайным образом отбрасывает 10% существующих соединений.
    Dense(10,  activation='softmax')
])

model_res.compile(optimizer='SGD',
              loss=keras.losses.categorical_crossentropy,
              metrics=['accuracy'])

model_res.fit(X_train, y_train,
          batch_size=64,
          epochs=15,
          validation_data=[X_val, y_val],
          callbacks=[tensorboard_callback])

from sklearn.metrics import accuracy_score
test_acc = accuracy_score(y_test, np.argmax(model_res.predict(X_test), axis=1))
print("\n Test_acc =", test_acc)
assert test_acc > 0.6939, "Not good enough. Back to the drawing board :)"
print(" Not bad!")
print("\n Изменение модели дало +", round(((test_acc-0.6939)/0.6939)*100),"% точности")

model_res.summary()

"""### Увеличиваем количество полносвязных слоёв"""

model_res = Sequential([
    Conv2D(20, (3,3), padding='same', activation='relu', input_shape=(32, 32, 3)), #20 фильтров с ядрами 3х3 пиксела каждый; выходная карта признаков на каждом канале 32*32
    BatchNormalization(),
    MaxPooling2D((2, 2), strides=2),
    BatchNormalization(),
    Conv2D(32, (3,3), padding='same', activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2), strides=2),
    BatchNormalization(),
    Conv2D(64, (3,3), padding='same', activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2), strides=2),
    BatchNormalization(),
    Flatten(),
    BatchNormalization(),
    Dense(128, activation='relu'),
    BatchNormalization(),
    Dense(256, activation='relu'),
    BatchNormalization(),
    Dense(512, activation='relu'),
    BatchNormalization(),
    L.Dropout(0.1), # исключающий слой для предотвращения переобучения, случайным образом отбрасывает 10% существующих соединений.
    Dense(10,  activation='softmax')
])

model_res.compile(optimizer='SGD',
              loss=keras.losses.categorical_crossentropy,
              metrics=['accuracy'])

model_res.fit(X_train, y_train,
          batch_size=64,
          epochs=15,
          validation_data=[X_val, y_val],
          callbacks=[tensorboard_callback])

from sklearn.metrics import accuracy_score
test_acc = accuracy_score(y_test, np.argmax(model_res.predict(X_test), axis=1))
print("\n Test_acc =", test_acc)
assert test_acc > 0.7004, "Not good enough. Back to the drawing board :)"
print(" Not bad!")
print("\n Изменение модели дало +", round(((test_acc-0.7004)/0.7004)*100),"% точности")

"""### меняем оптимизиатор"""

model_res = Sequential([
    Conv2D(20, (3,3), padding='same', activation='relu', input_shape=(32, 32, 3)), #20 фильтров с ядрами 3х3 пиксела каждый; выходная карта признаков на каждом канале 32*32
    BatchNormalization(),
    MaxPooling2D((2, 2), strides=2),
    BatchNormalization(),
    Conv2D(32, (3,3), padding='same', activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2), strides=2),
    BatchNormalization(),
    Conv2D(64, (3,3), padding='same', activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2), strides=2),
    BatchNormalization(),
    Flatten(),
    BatchNormalization(),
    Dense(100, activation='relu'),
    BatchNormalization(),
    Dense(10,  activation='softmax')
])

model_res.compile(optimizer='adam',
              loss=keras.losses.categorical_crossentropy,
              metrics=['accuracy'])

model_res.fit(X_train, y_train,
          batch_size=64,
          epochs=15,
          validation_data=[X_val, y_val],
          callbacks=[tensorboard_callback])

from sklearn.metrics import accuracy_score
test_acc = accuracy_score(y_test, np.argmax(model_res.predict(X_test), axis=1))
print("\n Test_acc =", test_acc)
assert test_acc > 0.6939, "Not good enough. Back to the drawing board :)"
print(" Not bad!")
print("\n Изменение модели дало +", round(((test_acc-0.6939)/0.6939)*100),"% точности")

"""### увеличиваем количество эпох"""

model_res.compile(optimizer='SGD',
              loss=keras.losses.categorical_crossentropy,
              metrics=['accuracy'])

model_res.fit(X_train, y_train,
          batch_size=64,
          epochs=35,
          validation_data=[X_val, y_val],
          callbacks=[tensorboard_callback])

from sklearn.metrics import accuracy_score
test_acc = accuracy_score(y_test, np.argmax(model_res.predict(X_test), axis=1))
print("\n Test_acc =", test_acc)
assert test_acc > 0.6939, "Not good enough. Back to the drawing board :)"
print(" Not bad!")
print("\n Изменение модели дало +", round(((test_acc-0.6939)/0.6939)*100),"% точности")

"""### увеличение свёрток, добавление Dropout и увеличение эпох"""

model_res = Sequential([
    Conv2D(20, (3,3), padding='same', activation='relu', input_shape=(32, 32, 3)), #20 фильтров с ядрами 3х3 пиксела каждый; выходная карта признаков на каждом канале 32*32
    BatchNormalization(),
    MaxPooling2D((2, 2), strides=2),
    BatchNormalization(),
    L.Dropout(0.1),
    Conv2D(32, (3,3), padding='same', activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2), strides=2),
    BatchNormalization(),
    L.Dropout(0.1),
    Conv2D(64, (3,3), padding='same', activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2), strides=2),
    BatchNormalization(),
    L.Dropout(0.1),
    Conv2D(128, (3,3), padding='same', activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2), strides=2),
    BatchNormalization(),
    L.Dropout(0.1),
    Flatten(),
    BatchNormalization(),
    Dense(256, activation='relu'),
    BatchNormalization(),
        Dense(10,  activation='softmax')
])

model_res.compile(optimizer='SGD',
              loss=keras.losses.categorical_crossentropy,
              metrics=['accuracy'])

model_res.fit(X_train, y_train,
          batch_size=64,
          epochs=40,
          validation_data=[X_val, y_val],
          callbacks=[tensorboard_callback])

from sklearn.metrics import accuracy_score
test_acc = accuracy_score(y_test, np.argmax(model_res.predict(X_test), axis=1))
print("\n Test_acc =", test_acc)
assert test_acc > 0.7274, "Not good enough. Back to the drawing board :)"
print(" Not bad!")
print("\n Изменение модели дало +", round(((test_acc-0.7274)/0.7274)*100),"% точности")

"""### меняем оптимизатор, увеличиваем эпохи"""

model_res.compile(optimizer='adam',
              loss=keras.losses.categorical_crossentropy,
              metrics=['accuracy'])

model_res.fit(X_train, y_train,
          batch_size=64,
          epochs=55,
          validation_data=[X_val, y_val],
          callbacks=[tensorboard_callback])

from sklearn.metrics import accuracy_score
test_acc = accuracy_score(y_test, np.argmax(model_res.predict(X_test), axis=1))
print("\n Test_acc =", test_acc)
assert test_acc > 0.7619, "Not good enough. Back to the drawing board :)"
print(" Not bad!")
print("\n Изменение модели дало +", round(((test_acc-0.7619)/0.7619)*100),"% точности")

!rm -rf ./logs

evaluator = Evaluator(model_res, optimizers=optimizers)
evaluator.fit(X_train, y_train, validation_data=(X_val, y_val))
evaluator.evaluate(X_test, y_test, accuracy_score)

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs/

"""### меняем оптимизатор, увеличиваем эпохи"""

model_res.compile(optimizer='SGD',
              loss=keras.losses.categorical_crossentropy,
              metrics=['accuracy'])

model_res.fit(X_train, y_train,
          batch_size=64,
          epochs=70,
          validation_data=[X_val, y_val],
          callbacks=[tensorboard_callback])

"""### Момент истины: проверьте, какого качества достигла ваша сеть."""

from sklearn.metrics import accuracy_score
test_acc = accuracy_score(y_test, np.argmax(model_res.predict(X_test), axis=1))
print("\n Test_acc =", test_acc)
if test_acc > 0.8:
    print("Это победа!")

"""Достичь требуемый порог качества удалось следующими изменениями в модели:
- увеличение количества сверточных слоёв и пуллинга;
- добавление дропаута;
- ~~изменение модели оптимизатора~~;
- увеличение количества эпох.
"""