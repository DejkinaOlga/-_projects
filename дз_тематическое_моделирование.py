# -*- coding: utf-8 -*-
"""КОР_ДЗ_Тематическое моделирование

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Fe5uuuEvFFtw2M8w4QvdDyg4M_KbV0-S

__Классификация по тональности__

В этом домашнем задании вам предстоит классифицировать по тональности отзывы на банки с сайта banki.ru.

Данные содержат непосредственно тексты отзывов, некоторую дополнительную информацию, а также оценку по шкале от 1 до 5.

Тексты хранятся в json-ах в массиве responses.

Посмотрим на примере отзыва: возьмите для удобства ноутбук, размещенный в папке репозитория.
"""

import json

import bz2
import regex
from tqdm import tqdm
from scipy import sparse

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import nltk
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
# %pylab inline

responses = []
with bz2.BZ2File('banki_responses.json.bz2', 'r') as thefile:
    for row in tqdm(thefile):
        resp = json.loads(row)
        if not resp['rating_not_checked'] and (len(resp['text'].split()) > 0):
            responses.append(resp)

responses[575]

"""# Часть 1. Анализ текстов

## 1. Посчитайте количество отзывов в разных городах и на разные банки
"""

df = pd.DataFrame(responses)
df.head()

df.shape

df.info()

df.isna().sum() # смотрим пропуски

df.bank_name.nunique() # количество уникальных банков

df.city.nunique() # количество уникальных городов

df.bank_name.value_counts(dropna=False).sort_index() # кол-во вхождений по банкам с сортировкой по алфавиту

sorted(df.bank_name.unique()) # смотрим банки "глазами"

"""На мой взгляд, с названиями банков всё в порядке. Встречается "задвоенность" (например, ВТБ - ВТБ 24 - ВТБ Северо-Запад). В целом, названия банков можно оставить в таком виде."""

df.city.value_counts(dropna=False).sort_index(ascending=True) # кол-во вхождений по городам с сортировкой по алфавиту и учётом None

"""Названия городов требуют обработки. Много пропущенных значений, скобки и т.п."""

#!pip install natasha
# natasha — это библиотека для извлечения именованных сущностей на основе Yargy-парсера и CRF, плюс некоторое количество предобученных моделей,
# например, для извлечения имен.

#!pip install natasha<1 yargy<0.13 # Чтобы использовать старый NamesExtractor, AddressExtactor понизьте версию pip install natasha<1 yargy<0.13

!pip install "natasha<1" "yargy<0.13"

from natasha import LocationExtractor

def extract_city(text):
    if isinstance(text, str):
        extractor = LocationExtractor()
        matches = extractor(text)
        if len(matches) > 0:
            return matches[0].fact.name
        else:
            return None
    else:
        return None

df['city_copy_1'] = df['city'].map(extract_city)

df.head(1)

df[['city','city_copy_1']].head(20)

print(df.city.nunique()) # количество уникальных городов ДО обработки
print('________________')
print(df.city_copy_1.nunique()) # количество уникальных городов ПОСЛЕ обработки

df.city_copy_1.value_counts(dropna=False).sort_index(ascending=True) # кол-во вхождений по городам с сортировкой по алфавиту и учётом None

print(df.city.isna().sum()) # пропуски в городах ДО обработки
print('----------')
print(df.city_copy_1.isna().sum()) # # пропуски в городах ПОСЛЕ обработки

print('Если удалим все пропуски по городам, то потеряем', round(100-((153499-18419)/153499)*100), '%, это чуть больше допустимых 10%')

df=df.drop(['city'], axis=1) # удалили не нужные далее столбцы

df_=df.dropna(subset=['city_copy_1'], axis=0) # удалили пропуски
df_.info()

sorted(df_.city_copy_1.unique()) # смотрим обработанные города "глазами"

"""Результат не очень.

Например, Москва присутствует в таких вариантах: 'москва', 'москваа', 'москвабад', 'москвай', 'москваха'. Есть город "ёсаратов". Встречаются такие как 'город тула'. Также вместо городов встречаются названия областей и районов.

Но, конечно, лучше чем было.

Дальше в задании я не вижу задач, связанных с названиями городов, поэтому не стоит делать более глубокую обработку городов.


"""

# количество отзывов в разных городах и на разные банки
df_count = df_.groupby(['bank_name', 'city_copy_1']).size().reset_index(name='Count').sort_values(by='Count', ascending=False)
df_count

"""Далее сокращу объем датасета, оставив города с максимальным количеством отзывов."""

df_city_count = df_.groupby(['city_copy_1']).size().reset_index(name='Count').sort_values(by='Count', ascending=False)
df_city_count

df_new=df_.loc[df_.city_copy_1.isin(['москва', 'петербург','новосибирск','екатеринбург'])]

df_new.head()

df_new.city_copy_1.nunique()

df_new.shape[0]

df_new.to_csv (r'export_dataframe.csv', index = None, header=True) #для удобства сохраню обработанный датафрейм

"""## 2. Постройте гистограмы длин слов в символах и в словах

Думаю, опечатка в задании.

Нужно построить гистограмму количества слов в отзыве и гистограмму количества симолов в отзыве (из ответа преподавателя в чате).

### гистограмма по количеству символов
"""

df_1 = pd.read_csv('export_dataframe.csv')

df_1=df_1.iloc[0:11000] # ещё уменьшу объем датафрейма, иначе слишком долго обрабатывается

df_1.shape

df_1.to_csv (r'new_export_dataframe.csv', index = None, header=True)

df_1 = pd.read_csv('new_export_dataframe.csv')

from tqdm._tqdm_notebook import tqdm_notebook

tqdm_notebook.pandas()

# количество символов в отзыве
df_1['text_len'] = df_1.text.progress_apply(len)
df_1.sort_values(by='text_len', ascending=False)

df_1.text_len.describe()

sns.histplot(data=df_1, x="text_len")
plt.title('Гистограмма количества символов в отзыве');

"""### гистограмма по количеству слов"""

import re
regex = re.compile("[а-я]+")

def words_only(text, regex=regex):
    try:
        return " ".join(regex.findall(text))
    except:
        return ""

df_1['text_words'] = df_1['text'].str.lower().progress_apply(words_only)
df_1.head()

df_1['words_count'] = df_1['text_words'].progress_apply(lambda x: len(x.split()))
df_1.head(1)

df_1.words_count.describe()

sns.histplot(data=df_1, x='words_count')
plt.title('Гистограмма количества слов в отзыве');

"""## 3. Найдите 10 самых частых:

*   слов
*   слов без стоп-слов
*   лемм
*   существительных

### слова
"""

from nltk.tokenize import word_tokenize

import nltk
nltk.download('punkt')

from string import punctuation
punctuation

from collections import Counter
corpus = [token for tweet in df_1.text for token in word_tokenize(tweet) if token not in punctuation]

freq_dict = Counter(corpus) #составляем частотный словарь

freq_dict.most_common(10) # 10 самых частых слов

"""### слова без стоп-слов"""

import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
print(stopwords.words('russian'))

freq_dict.most_common(60) # просматриваю свои частотные слова, чтобы добавить в стопы

"""На 60-ти самых частых словах видно, что исключение пунктуации плохо сработало."""

noise = stopwords.words('russian') + list(punctuation)+['«','``',"''",'...','ст','м','руб','очень','это','наш','тыс','млн','млрд','также','т','д']

corpus_1 = [token for tweet in df_1.text for token in word_tokenize(tweet) if token not in noise]

freq_dict_1 = Counter(corpus_1) #составляем частотный словарь

freq_dict_1.most_common(60)

"""Мне не нравится результат. Поэтому для дальнейшей работы буду использовать столбец 'text_words',предобработанный через регулярку."""

def remove_stopwords(text, mystopwords = noise):
    try:
        return " ".join([token for token in text.split() if not token in mystopwords])
    except:
        return ""

df_1['text_without_stop_words'] = df_1['text_words'].progress_apply(remove_stopwords)

cnt_stop = Counter()
n_types_stop = []
n_tokens_stop = []
tokens_stop = []
for index, row in tqdm(df_1.iterrows(), total = len(df_1)):
    tokens_stop = row['text_without_stop_words'].split()
    cnt_stop.update(tokens_stop)
    n_types_stop.append(len(cnt_stop))
    n_tokens_stop.append(sum(list(cnt_stop.values())))
for i in cnt_stop.most_common(10):
    print(i) # 10 самых частых слов без стоп-слов

cnt_stop.most_common(100) # смотрю "глазами", чтобы отредактировать стоп-слова

noise = noise+['г','с','е','р','вс','хотя','моей','нужно','мои','сразу','вообще','буду','либо','пока','мной']

def remove_stopwords(text, mystopwords = noise):
    try:
        return " ".join([token for token in text.split() if not token in mystopwords])
    except:
        return ""

df_1['кор_text_without_stop_words'] = df_1['text_words'].progress_apply(remove_stopwords)

cnt_stop = Counter()
n_types_stop = []
n_tokens_stop = []
tokens_stop = []
for index, row in tqdm(df_1.iterrows(), total = len(df_1)):
    tokens_stop = row['кор_text_without_stop_words'].split()
    cnt_stop.update(tokens_stop)
    n_types_stop.append(len(cnt_stop))
    n_tokens_stop.append(sum(list(cnt_stop.values())))
for i in cnt_stop.most_common(10):
    print(i) # 10 самых частых слов без стоп-слов (обновлено)

cnt_stop.most_common(100) # проверяю результат "глазами"

"""Получился, конечно, не идеальный результат. Но, в целом, можно оставить)"""

df_1.info()

df_2=df_1.drop(['text_len','text_words','text_without_stop_words'], axis=1) # удалили не нужные далее столбцы

df_2.head(1)

df_1.to_csv (r'1_new_export_dataframe.csv', index = None, header=True)

df_2.to_csv (r'2_new_export_dataframe.csv', index = None, header=True)

"""### леммы"""

df_2 = pd.read_csv('2_new_export_dataframe.csv')

!pip install pymorphy2

import pymorphy2
m = pymorphy2.MorphAnalyzer()
# pymorphy2 работает с отдельными словами. Если дать ему на вход предложение - он его просто не лемматизирует, т.к. не понимает

def lemmatize(text, morph=m):
    try:
        return ' '.join([morph.parse(word)[0].normal_form for word in text.split()])
    except:
        return " "

df_2['lemma'] = df_2['кор_text_without_stop_words'].progress_apply(lemmatize)

cnt_lemmas = Counter()
n_types_lemmas = []
n_tokens_lemmas = []
tokens_lemmas = []
for index, row in tqdm(df_2.iterrows(), total = len(df_2)):
    tokens_lemmas = row['lemma'].split()
    cnt_lemmas.update(tokens_lemmas)
    n_types_lemmas.append(len(cnt_lemmas))
    n_tokens_lemmas.append(sum(list(cnt_lemmas.values())))
for i in cnt_lemmas.most_common(10): # 10 самых частых лемм
    print(i)

"""вроде бы нормально отработано, если не считать 'деньгА')))))"""

cnt_lemmas.most_common(60) # визуальная проверка)

df_2.to_csv (r'3_new_export_dataframe.csv', index = None, header=True)

"""### существительные"""

df_2 = pd.read_csv('3_new_export_dataframe.csv')

# pymorphy2 Часть речи можно получить через атрибут POS

p = m.parse('жена')[0]
p.tag.POS

cnt_noun = Counter()
n_types_noun = []
n_tokens_noun = []
tokens_noun = []
for index, row in tqdm(df_2.iterrows(), total = len(df_2)):
    tokens_noun = [m.parse(word)[0].normal_form for word in row['lemma'].split() if m.parse(word)[0].tag.POS == 'NOUN'] # NOUN -существительное (список граммем)
    cnt_noun.update(tokens_noun)
    n_types_noun.append(len(cnt_noun))
    n_tokens_noun.append(sum(list(cnt_noun.values())))
for i in cnt_noun.most_common(10): # 10 самых частых существительных
    print(i)

"""Надо бы исправить 'деньгУ'..."""

m.parse('деньги')[0].normal_form

m.parse('денег')[0].normal_form

m.parse('деньга')[0].inflect({'plur', 'nomn'}).word

"""Вычитала, что в pymorphy2 имена существительные употребляемые только во множественном числе должны быть с пометкой Pltm ("Pluralia tantum")"""

m.parse('деньги')

"""Пометки нет. Надо в словарь изменение вносить?

## 4. Постройте кривые Ципфа и Хипса

Кривые буду строить на лемматизированных данных с учётом всех частей речи

### кривая Ципфа
"""

freqs = list(cnt_lemmas.values())
freqs = sorted(freqs, reverse = True)

fig, ax = plt.subplots()
ax.plot(freqs[:300], range(300))
plt.grid()
plt.title('Кривая Ципфа')
plt.ylabel('частота слова')
plt.xlabel('ранг слова');

"""### кривая Хипса"""

fig, ax = plt.subplots()
ax.plot(n_tokens_lemmas, n_types_lemmas)
plt.grid()
plt.title('Кривая Хипса')
plt.ylabel('число уникальных слов в тексте')
plt.xlabel('размер текста');

"""## 5. Ответьте на следующие вопросы:
*   какое слово встречается чаще, "сотрудник" или "клиент"?
*   сколько раз встречается слова "мошенничество" и "доверие"?

### "сотрудник" или "клиент"?
"""

print('слово "сотрудник" встречается', cnt_lemmas['сотрудник'], 'раз')
print('слово "клиент" встречается', cnt_lemmas['клиент'], 'раз')

if cnt_lemmas['сотрудник'] > cnt_lemmas['клиент']:
      print('слово "сотрудник" встречается в', round(cnt_lemmas['сотрудник']/cnt_lemmas['клиент'],3), 'раз чаще, чем слово "клиент"')
else:
      print('слово "клиент" встречается в', round(cnt_lemmas['клиент']/cnt_lemmas['сотрудник'],3), 'раз чаще, чем слово "сотрудник"')

"""### "мошенничество" и "доверие"?"""

print('слово "мошенничество" встречается', cnt_lemmas['мошенничество'], 'раз')
print('слово "доверие" встречается', cnt_lemmas['доверие'], 'раз')

"""##  6. В поле "rating_grade" записана оценка отзыва по шкале от 1 до 5. Используйте меру tf−idf, для того, чтобы найти ключевые слова и биграмы для положительных отзывов (с оценкой 5) и отрицательных отзывов (с оценкой 1)"""

df_2['rating_grade'].unique()

df_2['rating_grade'].fillna(0, inplace=True) #заменили nan на 0

df_2.rating_grade.value_counts() # распределение оценок

from sklearn.feature_extraction.text import TfidfVectorizer

tokens_by_rating = []
for rating in range(6):
    print(rating)
    tokens = []
    sample = df_2[df_2['rating_grade']==rating]['lemma']

    for i in range(len(sample)):
        tokens += sample.iloc[i].split()
    tokens_by_rating.append(tokens)

"""### ключевые слова и биграмы для положительных отзывов"""

tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 0)
tfidf_matrix =  tfidf.fit_transform([' '.join(tokens) for tokens in tokens_by_rating])
feature_names = tfidf.get_feature_names_out()
tfidf_ranking5_uni = []
dense = tfidf_matrix.todense()

text = dense[5].tolist()[0]
phrase_scores = [pair for pair in zip(range(0, len(text)), text) if pair[1] > 0]
sorted_phrase_scores = sorted(phrase_scores, key=lambda t: t[1] * -1)
phrases = []
for phrase, score in [(feature_names[word_id], score) for (word_id, score) in sorted_phrase_scores][:40]:
    tfidf_ranking5_uni.append(phrase)

tfidf = TfidfVectorizer(analyzer='word', ngram_range=(2,2), min_df = 0)
tfidf_matrix =  tfidf.fit_transform([' '.join(tokens) for tokens in tokens_by_rating])
feature_names = tfidf.get_feature_names_out()
tfidf_ranking5_bi = []
dense = tfidf_matrix.todense()

text = dense[5].tolist()[0]
phrase_scores = [pair for pair in zip(range(0, len(text)), text) if pair[1] > 0]
sorted_phrase_scores = sorted(phrase_scores, key=lambda t: t[1] * -1)
phrases = []
for phrase, score in [(feature_names[word_id], score) for (word_id, score) in sorted_phrase_scores][:40]:
    tfidf_ranking5_bi.append(phrase)

rankings5 = pd.DataFrame({'ключевые слова положительных отзывов': tfidf_ranking5_uni, 'биграмы положительных отзывов': tfidf_ranking5_bi})
rankings5.head(20)

"""### ключевые слова и биграмы для отрицательных отзывов"""

tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 0)
tfidf_matrix =  tfidf.fit_transform([' '.join(tokens) for tokens in tokens_by_rating])
feature_names = tfidf.get_feature_names_out()
tfidf_ranking1_uni = []
dense = tfidf_matrix.todense()

text = dense[1].tolist()[0]
phrase_scores = [pair for pair in zip(range(0, len(text)), text) if pair[1] > 0]
sorted_phrase_scores = sorted(phrase_scores, key=lambda t: t[1] * -1)
phrases = []
for phrase, score in [(feature_names[word_id], score) for (word_id, score) in sorted_phrase_scores][:40]:
    tfidf_ranking1_uni.append(phrase)

tfidf = TfidfVectorizer(analyzer='word', ngram_range=(2,2), min_df = 0)
tfidf_matrix =  tfidf.fit_transform([' '.join(tokens) for tokens in tokens_by_rating])
feature_names = tfidf.get_feature_names_out()
tfidf_ranking1_bi = []
dense = tfidf_matrix.todense()

text = dense[1].tolist()[0]
phrase_scores = [pair for pair in zip(range(0, len(text)), text) if pair[1] > 0]
sorted_phrase_scores = sorted(phrase_scores, key=lambda t: t[1] * -1)
phrases = []
for phrase, score in [(feature_names[word_id], score) for (word_id, score) in sorted_phrase_scores][:40]:
    tfidf_ranking1_bi.append(phrase)

rankings1 = pd.DataFrame({'ключевые слова отрицательных отзывов': tfidf_ranking1_uni, 'биграмы отрицательных отзывов': tfidf_ranking1_bi})
rankings1.head(20)

"""# Часть 2. Тематическое моделирование

## 1. Постройте несколько тематических моделей коллекции документов с разным числом тем. Приведите примеры понятных (интерпретируемых) тем.
"""

import gensim.corpora as corpora
from gensim.models import *

from sklearn.metrics import *
from sklearn.pipeline import *
from sklearn.feature_extraction.text import *
from sklearn.decomposition import TruncatedSVD, PCA
from sklearn.preprocessing import Normalizer, LabelEncoder

import warnings
warnings.filterwarnings('ignore')

texts = [df_2['lemma'].iloc[i].split() for i in range(len(df_2))]
dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

# Commented out IPython magic to ensure Python compatibility.
print("Тематическая модель с 10 темами")

# %time
lda_10 = ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, chunksize=50, update_every=1, passes=2, random_state=42, alpha='auto', eta='auto')
# num_topics ( int , необязательный ) — количество запрошенных скрытых тем, которые необходимо извлечь из учебного корпуса.
# chunksize ( int , необязательный ) — количество документов, которые будут использоваться в каждом обучающем фрагменте.
# update_every ( int , необязательный ) — количество документов, которые необходимо перебирать для каждого обновления. Установите 0 для пакетного обучения, > 1 для интерактивного итеративного обучения.
# pass ( int , необязательный ) — количество проходов по корпусу во время обучения.
# alpha – Априорное убеждение в распространении темы документа. 'auto': изучает асимметричный априор из корпуса (недоступно, если Distributed==True ).
# eta - Априорное убеждение в распределении слов по теме.'auto': изучает асимметричный априор из корпуса.

lda_10.show_topics(num_topics=3, num_words=20, formatted=False)
# num_topics ( int , необязательный ) — количество возвращаемых тем. В отличие от LSA, в LDA нет естественного порядка между темами. Таким образом, возвращаемое подмножество тем для всех тем является произвольным и может измениться между двумя прогонами обучения LDA.
# num_words ( int , необязательный ) — количество слов, которые будут представлены для каждой темы.
# formatted ( bool , необязательный ) — должны ли представления темы быть отформатированы как строки. Если False, они возвращаются как 2 кортежа (слово, вероятность).

"""По темам __модели lda_10__ мысли такие:

тема №0. затрудняюсь...

тема №3. новогодние вклады в отделении банка?

тема №7. услуги онлайн-банка?
"""

# Commented out IPython magic to ensure Python compatibility.
print("Тематическая модель с 20 темами")

# %time
lda_20 = ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=20, chunksize=50, update_every=1, passes=2, random_state=42, alpha='auto', eta='auto')

lda_20.show_topics(num_topics=3, num_words=20, formatted=False)

"""По __модели lda_20__ понятная тема:

тема №18. долгое обслуживание в отделениях банка
"""

# Commented out IPython magic to ensure Python compatibility.
print("Тематическая модель с 30 темами")

# %time
lda_30 = ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=30, chunksize=50, update_every=1, passes=2, random_state=42, alpha='auto', eta='auto')

lda_30.show_topics(num_topics=3, num_words=30, formatted=False)

"""По __модели lda_30__ интересная тема №27 - какой-то скандал в реннесансе из кукуевска?)))"""

# Commented out IPython magic to ensure Python compatibility.
print("Тематическая модель со 100 темами")

# %time
lda_100 = ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=100, chunksize=50, update_every=1, passes=2, random_state=42, alpha='auto', eta='auto')

lda_100.show_topics(num_topics=3, num_words=30, formatted=False)

"""## 2. Найдите темы, в которых упомянуты конкретные банки (Сбербанк, ВТБ, другой банк). Можете ли вы их прокомментировать / объяснить? Эта часть задания может быть сделана с использованием gensim."""

lda_10.get_term_topics('сбербанк') # Получить наиболее релевантные темы для данного слова.
# word_id ( int ) — слово, для которого будет вычислено распределение тем.
# Minimum_probability ( float , необязательный ) — Темы с назначенной вероятностью ниже этого порога будут отброшены.
# Возвращает: Релевантные темы представлены в виде пар их идентификаторов и назначенных им вероятностей, отсортированных по релевантности данному слову.

lda_100.get_term_topics('сбербанк')

lda_100.print_topic(10, topn=30) # Получить одну тему в виде форматированной строки.

"""тема №10 из lda_100, в которой упомянут сбербанк, скорее всего о перебоях в онлайн-банке"""

lda_100.get_term_topics('втб')

lda_100.print_topic(5, topn=30)

"""тема №5 из lda_100, в которой упомянут ВТБ, затрудняюсь... как записаться к руководству ВТБ?"""

lda_100.get_term_topics('банк')

"""# Часть 3. Классификация текстов

Сформулируем для простоты задачу бинарной классификации: будем классифицировать на два класса, то есть, различать резко отрицательные отзывы (с оценкой 1) и положительные отзывы (с оценкой 5).

## 1. Составьте обучающее и тестовое множество: выберите из всего набора данных N1 отзывов с оценкой 1 и N2 отзывов с оценкой 5 (значение N1 и N2 – на ваше усмотрение). Используйте sklearn.model_selection.train_test_split для разделения множества отобранных документов на обучающее и тестовое.
"""

df_2.info()

df_N1_N2=df_2.loc[df_2.rating_grade.isin([1,5])]

df_N1_N2.rating_grade.value_counts(dropna=False) # смотрим сбалансированность данных

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

le.fit(df_N1_N2.rating_grade)

le.transform( ['1', '5']) # пример расшифровки

y = pd.Series (le.transform(df_N1_N2.rating_grade))
y.head(10)

X=df_N1_N2.lemma

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(df_N1_N2.shape[0])
print(X_train.shape)
print(X_test.shape)

"""## 2. Используйте любой известный вам алгоритм классификации текстов для решения задачи и получите baseline. Сравните разные варианты векторизации текста: использование только униграм, пар или троек слов или с использованием символьных -грам."""

from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import CountVectorizer

from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report, confusion_matrix

vec_1 = CountVectorizer(ngram_range=(1, 1)) # униграмы
bow_1 = vec_1.fit_transform(X_train) # bow -- bag of words (мешок слов)

clf_1 = LogisticRegression(random_state=42, solver='liblinear')
clf_1.fit(bow_1, y_train)

pred_1 = clf_1.predict(vec_1.transform(X_test))
print(classification_report(pred_1, y_test, target_names=['негатив (оценка 1)', 'позитив (оценка 5)']))

vec_2 = CountVectorizer(ngram_range=(2, 2)) # биграмы
bow_2 = vec_2.fit_transform(X_train)
clf_2 = LogisticRegression(random_state=42)
clf_2.fit(bow_2, y_train)
pred_2 = clf_2.predict(vec_2.transform(X_test))
print(classification_report(pred_2, y_test, target_names=['негатив (оценка 1)', 'позитив (оценка 5)']))

vec_3 = CountVectorizer(ngram_range=(3, 3)) # триграмы
bow_3 = vec_3.fit_transform(X_train)
clf_3 = LogisticRegression(random_state=42)
clf_3.fit(bow_3, y_train)
pred_3 = clf_3.predict(vec_3.transform(X_test))
print(classification_report(pred_3, y_test, target_names=['негатив (оценка 1)', 'позитив (оценка 5)']))

report_1 = classification_report(y_test, pred_1, output_dict=True)
df_r_1 = pd.DataFrame(report_1)
report_2 = classification_report(y_test, pred_2, output_dict=True)
df_r_2 = pd.DataFrame(report_2)
report_3 = classification_report(y_test, pred_3, output_dict=True)
df_r_3 = pd.DataFrame(report_3)

test_1=pd.Series(df_r_1[2:3]['macro avg'], name='униграмы')
test_1=pd.DataFrame(test_1)
test_2=pd.Series(df_r_2[2:3]['macro avg'], name='биграмы')
test_2=pd.DataFrame(test_2)
test_3=pd.Series(df_r_3[2:3]['macro avg'], name='триграмы')
test_3=pd.DataFrame(test_3)
test_concat = pd.concat([test_1, test_2,test_3], axis=1)

test_concat

"""Лучший результат показали униграмы. Это может быть связано с тем, что я использовала сильно уменьшенный объем данных.

## 3. Сравните, как изменяется качество решения задачи при использовании скрытых тем в качестве признаков:


*   1-ый вариант: tf−idf преобразование (sklearn.feature_extraction.text.TfidfTransformer) и сингулярное разложение (оно же – латентый семантический анализ) (sklearn.decomposition.TruncatedSVD),
*   2-ой вариант: тематические модели LDA (sklearn.decomposition.LatentDirichletAllocation). Используйте accuracy и F-measure для оценки качества классификации.


В ноутбуке, размещенном в папке репозитория. написан примерный Pipeline для классификации текстов.

Эта часть задания может быть сделана с использованием sklearn.
"""

from sklearn.pipeline import Pipeline # pipeline позволяет объединить в один блок трансформер и модель, что упрощает написание кода и улучшает его читаемость
from sklearn.feature_extraction.text import TfidfVectorizer # TfidfVectorizer преобразует тексты в числовые вектора, отражающие важность использования каждого слова из некоторого набора слов (количество слов набора определяет размерность вектора) в каждом тексте
from sklearn.model_selection import RandomizedSearchCV
from sklearn import metrics
from sklearn.model_selection import GridSearchCV # модуль поиска по сетке параметров
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation
from sklearn.ensemble import RandomForestClassifier

"""### c класификатором из п.3.2

Чтобы создать конвейер, нам нужен список пар (ключ, значение), где ключ — это строка, содержащая то, что вы хотите назвать шагом, а значение — это объект оценки. Каждый шаг этого конвейера должен быть преобразователем, за исключением последнего шага, который может быть оценочного типа.
"""

ppl_clf_1 = Pipeline([
    ('vect', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('tm', TruncatedSVD()),
    ('clf', LogisticRegression(random_state=42))])
ppl_clf_2 = Pipeline([
    ('vect', CountVectorizer()),
    ('lda', LatentDirichletAllocation()),
    ('clf', LogisticRegression(random_state=42))])

ppl_clf_1.fit(X_train, y_train)
ppl_clf_2.fit(X_train, y_train)

pred_ppl_clf_1 = ppl_clf_1.predict(X_test)
print(classification_report(pred_ppl_clf_1, y_test, target_names=['негатив (оценка 1)', 'позитив (оценка 5)']))

pred_ppl_clf_2 = ppl_clf_2.predict(X_test)
print(classification_report(pred_ppl_clf_2, y_test, target_names=['негатив (оценка 1)', 'позитив (оценка 5)']))

"""В связи с тем, что датасет не сбалансирован, метрику «accuracy» (доля верных ответов) использовать нельзя, так как это приведет к завышенной оценке качества работы классификатора.

Для сравнения двух классификаторов использую «macro-avg» (точность, полнота и f-мера).

Лучший результат показал классификатор с использованием LDA (0,62 против 0,45).

Теперь сравним результаты с вариантами векторизации текста.
"""

report_4 = classification_report(y_test, pred_ppl_clf_1, output_dict=True)
df_r_4 = pd.DataFrame(report_4)
report_5 = classification_report(y_test, pred_ppl_clf_2, output_dict=True)
df_r_5 = pd.DataFrame(report_5)

test_4=pd.Series(df_r_4[2:3]['macro avg'], name='tf-idf + LSI')
test_4=pd.DataFrame(test_4)
test_5=pd.Series(df_r_5[2:3]['macro avg'], name='LDA')
test_5=pd.DataFrame(test_5)
test_concat_1 = pd.concat([test_concat, test_4,test_5], axis=1)

test_concat_1

"""Качество решения задачи ухудшилось

### c класификатором SGDClassifier
"""

from sklearn.linear_model import SGDClassifier

sgd_ppl_clf_1 = Pipeline([
    ('vect', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('tm', TruncatedSVD()),
    ('sgd_clf', SGDClassifier(random_state=42))])
sgd_ppl_clf_2 = Pipeline([
    ('vect', CountVectorizer()),
    ('lda', LatentDirichletAllocation()),
    ('sgd_clf', SGDClassifier(random_state=42))])

sgd_ppl_clf_1.fit(X_train, y_train)
sgd_ppl_clf_2.fit(X_train, y_train)

pred_sgd_ppl_clf_1 = sgd_ppl_clf_1.predict(X_test)
print(classification_report(pred_sgd_ppl_clf_1, y_test, target_names=['негатив (оценка 1)', 'позитив (оценка 5)']))

pred_sgd_ppl_clf_2 = sgd_ppl_clf_2.predict(X_test)
print(classification_report(pred_sgd_ppl_clf_2, y_test, target_names=['негатив (оценка 1)', 'позитив (оценка 5)']))

"""Лучший результат показал классификатор с использованием LDA (0,69 против 0,54).

Классификатор был обучен с параметрами по умолчанию. Попробуем подобрать оптимальные параметры модели через GridSearchCV. Для доступа к параметрам объекта pipeline необходимо указывать их в виде «название объекта»__«название параметра».
"""

parameters = {
              'sgd_clf__loss':['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],
              'sgd_clf__class_weight':[None, 'balanced'],
              'sgd_clf__penalty':[None, 'l2', 'l1', 'elasticnet']
              #'tfidf__strip_accents':['ascii', 'unicode', None]
              #'tfidf__ngram_range':[(1,2), (2,3), (3,4)]
              }
model_1 = GridSearchCV(sgd_ppl_clf_1, parameters, cv=4, n_jobs=-1).fit(X_train, y_train)
model_2 = GridSearchCV(sgd_ppl_clf_2, parameters, cv=4, n_jobs=-1).fit(X_train, y_train)
print('Оптимальные параметры для sgd_ppl_clf_1:')
print(model_1.best_score_, model_1.best_params_)
print('Оптимальные параметры для sgd_ppl_clf_2:')
print(model_2.best_score_, model_2.best_params_)

sgd_ppl_clf_1 = Pipeline([
    ('vect', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('tm', TruncatedSVD()),
    ('sgd_clf', SGDClassifier(random_state=42, class_weight=None, loss='hinge', penalty=None))])
sgd_ppl_clf_2 = Pipeline([
    ('vect', CountVectorizer()),
    ('lda', LatentDirichletAllocation()),
    ('sgd_clf', SGDClassifier(random_state=42, class_weight=None, loss='log', penalty='elasticnet'))])

sgd_ppl_clf_1.fit(X_train, y_train)
sgd_ppl_clf_2.fit(X_train, y_train)

pred_sgd_ppl_clf_1 = sgd_ppl_clf_1.predict(X_test)
print(classification_report(pred_sgd_ppl_clf_1, y_test, target_names=['негатив (оценка 1)', 'позитив (оценка 5)']))

pred_sgd_ppl_clf_2 = sgd_ppl_clf_2.predict(X_test)
print(classification_report(pred_sgd_ppl_clf_2, y_test, target_names=['негатив (оценка 1)', 'позитив (оценка 5)']))

"""Показатели стали хуже

### c класификатором RandomForestClassifier
"""

ppl_clf_1 = Pipeline([
    ('vect', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('tm', TruncatedSVD()),
    ('clf', RandomForestClassifier())])
ppl_clf_2 = Pipeline([
    ('vect', CountVectorizer()),
    ('lda', LatentDirichletAllocation()),
    ('clf', RandomForestClassifier())])

ppl_clf_1.fit(X_train, y_train)
ppl_clf_2.fit(X_train, y_train)

pred_ppl_clf_1 = ppl_clf_1.predict(X_test)
print(classification_report(pred_ppl_clf_1, y_test, target_names=['негатив (оценка 1)', 'позитив (оценка 5)']))

pred_ppl_clf_2 = ppl_clf_2.predict(X_test)
print(classification_report(pred_ppl_clf_2, y_test, target_names=['негатив (оценка 1)', 'позитив (оценка 5)']))

"""Лучший результат показал классификатор с использованием LDA (0,69 против 0,54)."""